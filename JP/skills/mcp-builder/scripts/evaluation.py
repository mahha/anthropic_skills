"""MCPã‚µãƒ¼ãƒãƒ¼è©•ä¾¡ãƒãƒ¼ãƒã‚¹

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ãƒ†ã‚¹ãƒˆè³ªå•ã‚’MCPã‚µãƒ¼ãƒãƒ¼ã«å¯¾ã—ã¦å®Ÿè¡Œã—ã€Claudeã§è©•ä¾¡ã—ã¾ã™ã€‚
"""

import argparse
import asyncio
import json
import re
import sys
import time
import traceback
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Any

from anthropic import Anthropic

from connections import create_connection

EVALUATION_PROMPT = """ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã‚¿ã‚¹ã‚¯ãŒä¸ãˆã‚‰ã‚ŒãŸã‚‰ã€å¿…ãšä»¥ä¸‹ã‚’è¡Œã£ã¦ãã ã•ã„:
1. åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã™ã‚‹
2. ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å„ã‚¹ãƒ†ãƒƒãƒ—ã®è¦ç´„ã‚’ <summary> ã‚¿ã‚°ã§å›²ã‚“ã§æç¤ºã™ã‚‹
3. æä¾›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã¸ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ <feedback> ã‚¿ã‚°ã§å›²ã‚“ã§æç¤ºã™ã‚‹
4. æœ€çµ‚å›ç­”ã‚’ <response> ã‚¿ã‚°ã§å›²ã‚“ã§æç¤ºã™ã‚‹

è¦ç´„ï¼ˆSummaryï¼‰ã®è¦ä»¶:
- <summary> ã‚¿ã‚°å†…ã§ã¯ã€æ¬¡ã‚’èª¬æ˜ã—ã¦ãã ã•ã„:
  - ã‚¿ã‚¹ã‚¯å®Œäº†ã®ãŸã‚ã«è¡Œã£ãŸæ‰‹é †
  - ã©ã®ãƒ„ãƒ¼ãƒ«ã‚’ã€ã©ã®é †ç•ªã§ã€ãªãœä½¿ã£ãŸã‹
  - å„ãƒ„ãƒ¼ãƒ«ã«æ¸¡ã—ãŸå…¥åŠ›
  - å„ãƒ„ãƒ¼ãƒ«ã‹ã‚‰å¾—ãŸå‡ºåŠ›
  - ã©ã®ã‚ˆã†ã«ã—ã¦å›ç­”ã«åˆ°é”ã—ãŸã‹ã®è¦ç´„

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆFeedbackï¼‰ã®è¦ä»¶:
- <feedback> ã‚¿ã‚°å†…ã§ã¯ã€ãƒ„ãƒ¼ãƒ«ã«å¯¾ã™ã‚‹å»ºè¨­çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¦ãã ã•ã„:
  - ãƒ„ãƒ¼ãƒ«åã«ã¤ã„ã¦: åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜çš„ã‹
  - å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ååˆ†ã‹ã€‚å¿…é ˆ/ä»»æ„ã®åŒºåˆ¥ã¯æ˜ç¢ºã‹
  - èª¬æ˜æ–‡ã«ã¤ã„ã¦: ãƒ„ãƒ¼ãƒ«ã®å‹•ä½œã‚’æ­£ç¢ºã«èª¬æ˜ã—ã¦ã„ã‚‹ã‹
  - ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ä¸­ã«é­é‡ã—ãŸã‚¨ãƒ©ãƒ¼ã«ã¤ã„ã¦: å®Ÿè¡Œå¤±æ•—ã‚„ãƒˆãƒ¼ã‚¯ãƒ³éå¤šãªã©ã¯ã‚ã£ãŸã‹
  - æ”¹å–„ç‚¹ã‚’å…·ä½“çš„ã«æŒ™ã’ã€ãªãœæœ‰ç”¨ã‹ï¼ˆWHYï¼‰ã‚‚èª¬æ˜ã™ã‚‹
  - å…·ä½“çš„ã‹ã¤å®Ÿè¡Œå¯èƒ½ãªææ¡ˆã«ã™ã‚‹

å›ç­”ï¼ˆResponseï¼‰ã®è¦ä»¶:
- å›ç­”ã¯ç°¡æ½”ã«ã—ã€è³ªå•ã«ç›´æ¥ç­”ãˆã¦ãã ã•ã„
- æœ€çµ‚å›ç­”ã¯å¿…ãš <response> ã‚¿ã‚°ã§å›²ã‚“ã§ãã ã•ã„
- è§£æ±ºã§ããªã„å ´åˆã¯ <response>NOT_FOUND</response> ã‚’è¿”ã—ã¦ãã ã•ã„
- æ•°å€¤å›ç­”ã¯æ•°å€¤ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„
- IDã¯IDã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„
- åå‰/ãƒ†ã‚­ã‚¹ãƒˆã¯è¦æ±‚ã•ã‚ŒãŸæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾è¿”ã—ã¦ãã ã•ã„
- <response> ã¯æœ€å¾Œã«ç½®ã„ã¦ãã ã•ã„"""


def parse_evaluation_file(file_path: Path) -> list[dict[str, Any]]:
    """qa_pairè¦ç´ ã‚’å«ã‚€XMLè©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¾ã™ã€‚"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        evaluations = []

        for qa_pair in root.findall(".//qa_pair"):
            question_elem = qa_pair.find("question")
            answer_elem = qa_pair.find("answer")

            if question_elem is not None and answer_elem is not None:
                evaluations.append({
                    "question": (question_elem.text or "").strip(),
                    "answer": (answer_elem.text or "").strip(),
                })

        return evaluations
    except Exception as e:
        print(f"è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        return []


def extract_xml_content(text: str, tag: str) -> str | None:
    """XMLã‚¿ã‚°ã‹ã‚‰å†…å®¹ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"""
    pattern = rf"<{tag}>(.*?)</{tag}>"
    matches = re.findall(pattern, text, re.DOTALL)
    return matches[-1].strip() if matches else None


async def agent_loop(
    client: Anthropic,
    model: str,
    question: str,
    tools: list[dict[str, Any]],
    connection: Any,
) -> tuple[str, dict[str, Any]]:
    """MCPãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"""
    messages = [{"role": "user", "content": question}]

    response = await asyncio.to_thread(
        client.messages.create,
        model=model,
        max_tokens=4096,
        system=EVALUATION_PROMPT,
        messages=messages,
        tools=tools,
    )

    messages.append({"role": "assistant", "content": response.content})

    tool_metrics = {}

    while response.stop_reason == "tool_use":
        tool_use = next(block for block in response.content if block.type == "tool_use")
        tool_name = tool_use.name
        tool_input = tool_use.input

        tool_start_ts = time.time()
        try:
            tool_result = await connection.call_tool(tool_name, tool_input)
            tool_response = json.dumps(tool_result) if isinstance(tool_result, (dict, list)) else str(tool_result)
        except Exception as e:
            tool_response = f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ {tool_name}: {str(e)}\n"
            tool_response += traceback.format_exc()
        tool_duration = time.time() - tool_start_ts

        if tool_name not in tool_metrics:
            tool_metrics[tool_name] = {"count": 0, "durations": []}
        tool_metrics[tool_name]["count"] += 1
        tool_metrics[tool_name]["durations"].append(tool_duration)

        messages.append({
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use.id,
                "content": tool_response,
            }]
        })

        response = await asyncio.to_thread(
            client.messages.create,
            model=model,
            max_tokens=4096,
            system=EVALUATION_PROMPT,
            messages=messages,
            tools=tools,
        )
        messages.append({"role": "assistant", "content": response.content})

    response_text = next(
        (block.text for block in response.content if hasattr(block, "text")),
        None,
    )
    return response_text, tool_metrics


async def evaluate_single_task(
    client: Anthropic,
    model: str,
    qa_pair: dict[str, Any],
    tools: list[dict[str, Any]],
    connection: Any,
    task_index: int,
) -> dict[str, Any]:
    """æŒ‡å®šãƒ„ãƒ¼ãƒ«ç¾¤ã§1ã¤ã®QAãƒšã‚¢ã‚’è©•ä¾¡ã—ã¾ã™ã€‚"""
    start_time = time.time()

    print(f"ã‚¿ã‚¹ã‚¯{task_index + 1}: è³ªå•ã‚’å®Ÿè¡Œã—ã¾ã™: {qa_pair['question']}")
    response, tool_metrics = await agent_loop(client, model, qa_pair["question"], tools, connection)

    response_value = extract_xml_content(response, "response")
    summary = extract_xml_content(response, "summary")
    feedback = extract_xml_content(response, "feedback")

    duration_seconds = time.time() - start_time

    return {
        "question": qa_pair["question"],
        "expected": qa_pair["answer"],
        "actual": response_value,
        "score": int(response_value == qa_pair["answer"]) if response_value else 0,
        "total_duration": duration_seconds,
        "tool_calls": tool_metrics,
        "num_tool_calls": sum(len(metrics["durations"]) for metrics in tool_metrics.values()),
        "summary": summary,
        "feedback": feedback,
    }


REPORT_HEADER = """
# è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

## ã‚µãƒãƒª

- **Accuracy**: {correct}/{total} ({accuracy:.1f}%)
- **å¹³å‡ã‚¿ã‚¹ã‚¯æ‰€è¦æ™‚é–“**: {average_duration_s:.2f}s
- **ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šå¹³å‡ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å›æ•°**: {average_tool_calls:.2f}
- **ç·ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å›æ•°**: {total_tool_calls}

---
"""

TASK_TEMPLATE = """
### Task {task_num}

**è³ªå•**: {question}
**æ­£è§£ï¼ˆGround Truthï¼‰**: `{expected_answer}`
**å®Ÿå›ç­”**: `{actual_answer}`
**æ­£èª¤**: {correct_indicator}
**æ‰€è¦æ™‚é–“**: {total_duration:.2f}s
**ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—**: {tool_calls}

**è¦ç´„**
{summary}

**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**
{feedback}

---
"""


async def run_evaluation(
    eval_path: Path,
    connection: Any,
    model: str = "claude-3-7-sonnet-20250219",
) -> str:
    """MCPã‚µãƒ¼ãƒãƒ¼ã®ãƒ„ãƒ¼ãƒ«ç¾¤ã§è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"""
    print("ğŸš€ è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™")

    client = Anthropic()

    tools = await connection.list_tools()
    print(f"ğŸ“‹ MCPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ãƒ„ãƒ¼ãƒ«ã‚’{len(tools)}å€‹èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    qa_pairs = parse_evaluation_file(eval_path)
    print(f"ğŸ“‹ è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’{len(qa_pairs)}ä»¶èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    results = []
    for i, qa_pair in enumerate(qa_pairs):
        print(f"ã‚¿ã‚¹ã‚¯å‡¦ç†ä¸­ {i + 1}/{len(qa_pairs)}")
        result = await evaluate_single_task(client, model, qa_pair, tools, connection, i)
        results.append(result)

    correct = sum(r["score"] for r in results)
    accuracy = (correct / len(results)) * 100 if results else 0
    average_duration_s = sum(r["total_duration"] for r in results) / len(results) if results else 0
    average_tool_calls = sum(r["num_tool_calls"] for r in results) / len(results) if results else 0
    total_tool_calls = sum(r["num_tool_calls"] for r in results)

    report = REPORT_HEADER.format(
        correct=correct,
        total=len(results),
        accuracy=accuracy,
        average_duration_s=average_duration_s,
        average_tool_calls=average_tool_calls,
        total_tool_calls=total_tool_calls,
    )

    report += "".join([
        TASK_TEMPLATE.format(
            task_num=i + 1,
            question=qa_pair["question"],
            expected_answer=qa_pair["answer"],
            actual_answer=result["actual"] or "N/A",
            correct_indicator="âœ…" if result["score"] else "âŒ",
            total_duration=result["total_duration"],
            tool_calls=json.dumps(result["tool_calls"], indent=2),
            summary=result["summary"] or "N/A",
            feedback=result["feedback"] or "N/A",
        )
        for i, (qa_pair, result) in enumerate(zip(qa_pairs, results))
    ])

    return report


def parse_headers(header_list: list[str]) -> dict[str, str]:
    """'Key: Value'å½¢å¼ã®ãƒ˜ãƒƒãƒ€ãƒ¼æ–‡å­—åˆ—ã‚’dictã«ãƒ‘ãƒ¼ã‚¹ã—ã¾ã™ã€‚"""
    headers = {}
    if not header_list:
        return headers

    for header in header_list:
        if ":" in header:
            key, value = header.split(":", 1)
            headers[key.strip()] = value.strip()
        else:
            print(f"è­¦å‘Š: ä¸æ­£ãªãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç„¡è¦–ã—ã¾ã™: {header}")
    return headers


def parse_env_vars(env_list: list[str]) -> dict[str, str]:
    """'KEY=VALUE'å½¢å¼ã®ç’°å¢ƒå¤‰æ•°æ–‡å­—åˆ—ã‚’dictã«ãƒ‘ãƒ¼ã‚¹ã—ã¾ã™ã€‚"""
    env = {}
    if not env_list:
        return env

    for env_var in env_list:
        if "=" in env_var:
            key, value = env_var.split("=", 1)
            env[key.strip()] = value.strip()
        else:
            print(f"è­¦å‘Š: ä¸æ­£ãªç’°å¢ƒå¤‰æ•°ã‚’ç„¡è¦–ã—ã¾ã™: {env_var}")
    return env


async def main():
    parser = argparse.ArgumentParser(
        description="ãƒ†ã‚¹ãƒˆè³ªå•ã‚’ä½¿ã£ã¦MCPã‚µãƒ¼ãƒãƒ¼ã‚’è©•ä¾¡ã—ã¾ã™",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¾‹:
  # Evaluate a local stdio MCP server
  python evaluation.py -t stdio -c python -a my_server.py eval.xml

  # Evaluate an SSE MCP server
  python evaluation.py -t sse -u https://example.com/mcp -H "Authorization: Bearer token" eval.xml

  # Evaluate an HTTP MCP server with custom model
  python evaluation.py -t http -u https://example.com/mcp -m claude-3-5-sonnet-20241022 eval.xml
        """,
    )

    parser.add_argument("eval_file", type=Path, help="è©•ä¾¡XMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("-t", "--transport", choices=["stdio", "sse", "http"], default="stdio", help="ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒˆç¨®åˆ¥ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: stdioï¼‰")
    parser.add_argument("-m", "--model", default="claude-3-7-sonnet-20250219", help="ä½¿ç”¨ã™ã‚‹Claudeãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: claude-3-7-sonnet-20250219ï¼‰")

    stdio_group = parser.add_argument_group("stdioã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    stdio_group.add_argument("-c", "--command", help="MCPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã™ã‚‹ã‚³ãƒãƒ³ãƒ‰ï¼ˆstdioã®ã¿ï¼‰")
    stdio_group.add_argument("-a", "--args", nargs="+", help="ã‚³ãƒãƒ³ãƒ‰å¼•æ•°ï¼ˆstdioã®ã¿ï¼‰")
    stdio_group.add_argument("-e", "--env", nargs="+", help="ç’°å¢ƒå¤‰æ•°ï¼ˆKEY=VALUEå½¢å¼ã€stdioã®ã¿ï¼‰")

    remote_group = parser.add_argument_group("sse/httpã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    remote_group.add_argument("-u", "--url", help="MCPã‚µãƒ¼ãƒãƒ¼URLï¼ˆsse/httpã®ã¿ï¼‰")
    remote_group.add_argument("-H", "--header", nargs="+", dest="headers", help="HTTPãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ'Key: Value'å½¢å¼ã€sse/httpã®ã¿ï¼‰")

    parser.add_argument("-o", "--output", type=Path, help="è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›å…ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: stdoutï¼‰")

    args = parser.parse_args()

    if not args.eval_file.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.eval_file}")
        sys.exit(1)

    headers = parse_headers(args.headers) if args.headers else None
    env_vars = parse_env_vars(args.env) if args.env else None

    try:
        connection = create_connection(
            transport=args.transport,
            command=args.command,
            args=args.args,
            env=env_vars,
            url=args.url,
            headers=headers,
        )
    except ValueError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    print(f"ğŸ”— {args.transport} ã§MCPã‚µãƒ¼ãƒãƒ¼ã¸æ¥ç¶šã—ã¾ã™...")

    async with connection:
        print("âœ… æ¥ç¶šã—ã¾ã—ãŸ")
        report = await run_evaluation(args.eval_file, connection, args.model)

        if args.output:
            args.output.write_text(report)
            print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {args.output}")
        else:
            print("\n" + report)


if __name__ == "__main__":
    asyncio.run(main())
